
                                                Master's Research Project
                Title: Predicting Relation Targets of DBpedia Properties Using Vector Representations from Word2vec
                                                         
                                                         
Abstract

Nowadays, the word2vec method is very popular in the field of machine learning to train vector representations of words, called word-embeddings. Vector representations of words have obtained enormous attention in natural language processing and information retrieval to detect word similarity, analogical reasoning and so on. They have been applied to predict relation targets and the word analogy task;  however,  only one pair of source and target has been used so far. Combining more than one pair of source and target in word-embeddings should theoretically strengthen the representative characteristic of the same category in a relation.  This project's aim is to increase the accuracy of analogical reasoning by aggregating semantic information of DBpedia data stored in word embeddings to predict relation targets. Therefore, we introduced two super vectors, which aggregate several vectors in the analogy task and can substantially increase the accuracy of predicting relation targets.  The following four relations, namely capital-country, currency-country, person-party and company-headquarter, are considered from DBpedia for this study to evaluate the system's performance. With the increasing number of vectors, the performance has increased for all of the above-described relations and the biggest improvement has been achieved for the company-headquarter relation.

For detailed information, please read Report_Relation_Prediction.pdf.
